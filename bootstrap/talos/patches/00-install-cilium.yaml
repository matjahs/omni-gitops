---
cluster:
  inlineManifests:
    - name: cilium-install-home
      contents: |-
        ########################################################
        # 1.  Namespace + values ConfigMap
        ########################################################
        apiVersion: v1
        kind: Namespace
        metadata:
          name: cilium
        ---
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: cilium-values
          namespace: cilium
        data:
          values.yaml: |
            # --- values cloned from former HelmRelease/patches ---
            # Cilium v1.17+ values.yaml schema - MODERNIZED for clarity and Helm best practice (2024-06)
            #   - IPAM uses 'operator.clusterPoolIPv4PodCIDRList'
            #   - 'devices' is now a YAML list for multi-nic/wildcard support
            #   - Comments inline for explain-all. Based on official Cilium Helm and upstream docs.
            #   - Reference: https://github.com/cilium/cilium/blob/v1.17.5/install/kubernetes/cilium/values.yaml

            # Kubernetes API server connection - REQUIRED for Cilium agent
            k8sServiceHost: 192.168.1.193
            k8sServicePort: 6443

            # ------------------------------------------------------------------
            # ðŸ‘‡ Cluster IPAM settings (modern form, 1.17+ schema recommended)
            # - Use operator.clusterPoolIPv4PodCIDRList for Pod IP allocation
            # - MaskSize controls pod subnetting (normally /24, should match node CIDR size)
            #
            ipam:
              mode: cluster-pool
              operator:
                clusterPoolIPv4PodCIDRList:
                  - 10.0.0.0/8
                clusterPoolIPv4MaskSize: 24  # Ensure this matches what Kubernetes advertises per-node
            #
            # ------------------------------------------------------------------

            # Native routing range - must include all PodCIDRs
            ipv4NativeRoutingCIDR: 10.0.0.0/8
            routingMode: native

            # --------- NATIVE ROUTING (table 200) settings ----------
            # Required for multi-node clusters to program direct node routes!
            # Use a list form for clarity, and enables wildcards for mixed-nic fleets
            devices:
              - enp1s0  # (can use 'en*', 'eth+', etc for wildcards, see Cilium docs)
            autoDirectNodeRoutes: true  # (MUST be named exactly as shown)
            # --------------------------------------------------------
            tunnelProtocol: geneve
            kubeProxyReplacement: true
            nodePort:
              enabled: true
              range: "30000,32767"
              directRoutingDevice: enp1s0  # this device will carry pod-to-pod direct routing traffic
            serviceLB:
              enabled: true
              ipam: pool
            # ------------------------------------------------------------------
            # Return-path behaviour (NAT â†’ DSR/Geneve)
            # ------------------------------------------------------------------
            loadBalancer:
              mode: dsr  # Direct-Server-Return
              dsrDispatch: geneve
            bgpControlPlane:
              enabled: true
            bgp:
              enabled: false
            upgradeCompatibility: "1.15"
            cleanState: false
            cleanBpfState: false
            nodeinit:
              enabled: false
            # securityContext equivalent of capfix.yaml
            securityContext:
              capabilities:
                ciliumAgent:
                  - CHOWN
                  - DAC_OVERRIDE
                  - FOWNER
                  - FSETID
                  - IPC_LOCK
                  - KILL
                  - MKNOD
                  - NET_ADMIN
                  - NET_RAW
                  - SETFCAP
                  - SETFCAP
                  - SETGID
                  - SETPCAP
                  - SETUID
                  - SYS_CHROOT
                  - SYS_RESOURCE
                  - BPF  # eBPF load
                  - PERFMON
                  - SYS_ADMIN  # still required by Cilium 1.17
                cleanCiliumState:
                  - NET_ADMIN
                  - SYS_ADMIN
                  - SYS_RESOURCE
            hubble:
              enabled: true
              relay:
                enabled: true
              ui:
                enabled: true
            gatewayAPI:
              enabled: true
              secretsNamespace:
                name: cilium
                create: false
                sync: true
            # ------------------------------------------------------------------
            # The following block was used to apply a *global allow-list* for
            # LoadBalancer allocations. Cilium's operator consults it *before*
            # it checks Service selectors or annotations.
            #
            # If you leave it uncommented, only the CIDRs listed here are
            # eligible for automatic ServiceLB allocation, which limits
            # flexibility when you want to add new LB pools or subnets at runtime.
            #
            # By commenting it out entirely, you let Cilium's operator
            # allocate LoadBalancer IPs from *any* defined pool, giving you
            # full flexibility. Pools can be managed via the CiliumLoadBalancerIPPool
            # CRDs without further `values.yaml` or Helm edits.
            #
            # (Retained for documentation and reference:)
            #
            # loadBalancerIPRanges:
            #   - cidr: 192.168.10.0/24
        ########################################################
        # 2. RBAC for the ephemeral install Job
        ########################################################
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: ClusterRoleBinding
        metadata:
          name: cilium-install
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: ClusterRole
          name: cluster-admin
        subjects:
          - kind: ServiceAccount
            name: cilium-install
            namespace: cilium
        ---
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: cilium-install
          namespace: cilium
        ########################################################
        # 3. Install Job (cilium-cli)
        ########################################################
        ---
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: cilium-install
          namespace: cilium
        spec:
          backoffLimit: 10
          template:
            metadata:
              labels: {app: cilium-install}
            spec:
              serviceAccountName: cilium-install
              hostNetwork: true  # lets the pod talk to the local API proxy
              restartPolicy: OnFailure
              tolerations:
                - operator: Exists
              affinity:  # pin to a control-plane node
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                      - matchExpressions:
                          - key: node-role.kubernetes.io/control-plane
                            operator: Exists
              containers:
                - name: cilium-install
                  image: quay.io/cilium/cilium-cli:v0.18.4
                  command:
                    - cilium
                    - install
                    - -f
                    - /values.yaml
                    - -n
                    - cilium
                    - --version
                    - 1.17.5  # pin to the same chart version you used
                  env:
                    # Use the node IP (pod IP on host network) instead of the
                    # Kubernetes service-clusterIP which isn't reachable yet.
                    - name: KUBERNETES_SERVICE_HOST
                      valueFrom:
                        fieldRef:
                          fieldPath: status.podIP
                    - name: KUBERNETES_SERVICE_PORT
                      value: "6443"
                  volumeMounts:
                    - name: cilium-values
                      mountPath: /values.yaml
                      subPath: values.yaml
              volumes:
                - name: cilium-values
                  configMap:
                    name: cilium-values
                    items:
                      - key: values.yaml
                        path: values.yaml
